---
layout: post
title: "The generalized eigenvalue problem"
description: "Interpreting generalized eigenvalue problems as comparative PCA"
tags:
giscus_comments: false
date: 2025-06-11
featured: false
categories: math
thumbnail:
related_posts: false
---

## Introduction

When I was just getting started in computational research, I often found myself exploring the [`scipy.linalg` documentation](https://docs.scipy.org/doc/scipy/reference/linalg.html) just to get a sense of what tools were available.
A critical tool I ended up using frequently was the function for symmetric matrix eigendecomposition, `scipy.linalg.eigh`, as it solves the eigenvalue problem underlying [principal component analysis (PCA)](/blog/2025/pca) and much of [graph signal processing](/blog/2025/graph-fourier).
Briefly, if $\mathbf{A}$ represented a covariance matrix, one could calculate the top PC by solving

$$
\mathbf{A} \mathbf{v} = \lambda \mathbf{v}.
$$

However, as I became more familiar with this function, I noticed that it optionally accepted a *second* matrix as input.
Given the two matrices, $\mathbf{A}$ and $\mathbf{B}$, it would apparently solve the [*generalized* eigenvalue problem](https://arxiv.org/abs/1903.11240):

$$
\begin{equation} \label{eq:geneigprob}
    \mathbf{A} \mathbf{v} = \lambda \mathbf{B} \mathbf{v}.
\end{equation}
$$

For a long time, I didn't have a great intuition for what this equation represented.
After watching [Arrival](https://en.wikipedia.org/wiki/Arrival_(film)) on a flight one time, I was inspired to make great scientific strides, but I ended up with the following interpretation of the generalized eigenvalue problem instead.

---


## Brief recap of PCA

As shown in a [previous post](/blog/2025/pca), PCA can be seen as reconstructing the graph of similarities between data points with the least amount of information.
We'll slightly change our approach to focus on the features here, but the results are the same whether you consider observations or features.
The original information spanning $n$ observations of $g$ features is given by the data matrix

$$
\mathbf{X} \in \mathbb{R}^{n \times g},
$$

which we will think of as $g$ genes measured across $n$ cells from a patient with a particular disease.

We are interested in a "reduced" representation given by

$$
\mathbf{v} \in \mathbb{R}^{g},
$$

which can be thought of as a data matrix with only a single column -- i.e. a "[metagene](https://www.pnas.org/doi/full/10.1073/pnas.0308531101)" or "[gene program](https://elifesciences.org/articles/43803)" -- and which we might assume is associated with the patient's disease.
Each representation yields relationships between features that we can think of as graphs (i.e. covariance matrices), which are given by

$$
\mathbf{X} \mathbf{X}^{\top}, \mathbf{v} \mathbf{v}^{\top} \in \mathbb{R}^{g \times g}.
$$

We can then quantify how well theses graphs match by comparing each edge weight via the summation

$$
\lambda = \sum_{ij} (\mathbf{X} \mathbf{X}^{\top})_{ij} (\mathbf{v} \mathbf{v}^{\top})_{ij},
$$

which boils down to the quadratic form

$$
\lambda = \mathbf{v}^{\top} \mathbf{A} \mathbf{v},
$$

where $\mathbf{A} = \mathbf{X} \mathbf{X}^{\top}$ is the adjacency matrix associated with the true data graph.

Normalizing by the magnitude of $\mathbf{v}$, we get

$$
\lambda = \frac{\mathbf{v}^{\top} \mathbf{C} \mathbf{v}}{\mathbf{v}^{\top} \mathbf{v}}.
$$

which simplifies down to an eigenvalue problem via

$$
\Rightarrow \lambda \mathbf{v}^{\top} \mathbf{v} = \mathbf{v}^{\top} \mathbf{C} \mathbf{v} \nonumber \\
$$

$$
\begin{equation} \label{eq:eigprob}
    \Rightarrow \lambda \mathbf{v} = \mathbf{C} \mathbf{v}.
\end{equation}
$$

Thus, we can find a "condensed representation" of the genes
With this intuition in mind, we can now "generalize" PCA to find relationships present in one graph, $\mathbf{A}$, but not another, $\mathbf{B}$.

---


## Generalization

In the single-cell analysis context mentioned above, the top gene programs (i.e. PCs) we identify would likely just describe critical functions present in *both healthy and disease* patients.
In order to deal with this, we would probably need to somehow specify that **we want to find patterns that are present uniquely in disease data and *not healthy data*.**

Consider we collected a second sample from a *healthy* patient that yielded the data matrix,

$$
\mathbf{Y} \in \mathbb{R}^{m \times g}.
$$

While the number of cells measured in this assay might differ from that of the first (i.e. $n\neq m$), we can still compare the two in terms of their features (hence why we are working with *feature* covariances in this post).
The graph of feature relationships for this healthy sample is given by

$$
\mathbf{B} = \mathbf{Y} \mathbf{Y}^{\top} \in \mathbb{R}^{g \times g}.
$$

Our goal is now to somehow remove the information in $\mathbf{B}$ from that of $\mathbf{A}$.
If we were doing this with *scalar* values associated with each dataset $a,b \in \mathbb{R}$, we could simply divide out $b$:

$$
\frac{a}{b}.
$$

But we are trying to do this with *matrices* $\mathbf{A}, \mathbf{B} \in \mathbb{R}^{g \times g}$, which we can't divide in the same way.
Nevertheless, we can think of division instead as **multiplication by an inverse**:

$$
b^{-1} a.
$$

This actually does generalize to matrices, so long as they are invertible.
Because $\mathbf{A}$ and $\mathbf{B}$ are positife semidefinite, they are invertible and we can calculate the inverse

$$
\mathbf{B}^{-1} \mathbf{A},
$$

which represents the information present in $\mathbf{A}$ but not $\mathbf{B}$.
Because this product is just another matrix itself, we should just be able to **plug it into an eigenvalue problem to find the dominant patterns, which should now be disease-specific:**

$$
\mathbf{B}^{-1} \mathbf{A} \mathbf{v} = \lambda \mathbf{v}.
$$

And that's it, there's our final equation and intuition.
All we need to do is **multiply each side by $\mathbf{B}$ and we arrive at our original eq. \eqref{eq:geneigprob}.**

One last thing I'd like to do is provide **intuition in the form of Rayleigh quotients.**
Just as we flipped between eigenvalue problem and Rayleigh quotient in eq. \eqnref{eq:eigprob}, so we can do with the generalized problem:

$$
\mathbf{C} \mathbf{v} = \lambda \mathbf{B} \mathbf{v}
$$

$$
\Rightarrow \mathbf{v}^{\top} \mathbf{C} \mathbf{v} = \lambda \mathbf{v}^{\top} \mathbf{B} \mathbf{v}\nonumber \\
$$

$$
\Rightarrow \lambda = \frac{\mathbf{v}^{\top} \mathbf{C} \mathbf{v}}{\mathbf{v}^{\top} \mathbf{B} \mathbf{v}}. \nonumber
$$

Here, we can see more clearly that the prominence of our component (i.e. the eigenvalue of $\mathbf{v}$, $\lambda$) is governed by how much it can emphasize the relationships in graph $\mathbf{A}$ **as well as how much it can *deemphasize* the relationships in graph $\mathbf{B}$.**

---


## Conclusion

Altogether, we have shown **two different ways in which generalized eigenvalue problems can be interpreted as a sort of comparative PCA.**
Note that this interpretation is also made clear in [linear discriminant analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis#Multiclass_LDA).
It can also be applied to canonical correlations analysis, as seen [here](https://cca-zoo.readthedocs.io/en/dev/documentation/maths.html) and [here](https://gregorygundersen.com/blog/2018/07/17/cca/).
Honestly, I'm surprised it hasn't totally abused in single-cell analysis to find disease-specific features, but maybe that's partially because [scRNA-seq data violates key distributional assumptions](https://www.nature.com/articles/s41592-023-01814-1).

Hope this was helpful nonetheless.
