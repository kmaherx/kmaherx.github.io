<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kmaherx.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kmaherx.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-12T13:57:43+00:00</updated><id>https://kmaherx.github.io/feed.xml</id><title type="html">blank</title><subtitle># A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Principal component analysis</title><link href="https://kmaherx.github.io/blog/2025/pca/" rel="alternate" type="text/html" title="Principal component analysis"/><published>2025-06-10T00:00:00+00:00</published><updated>2025-06-10T00:00:00+00:00</updated><id>https://kmaherx.github.io/blog/2025/pca</id><content type="html" xml:base="https://kmaherx.github.io/blog/2025/pca/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>It’s not a real blog unless I give my take on PCA.</p> <p>There are several other <a href="https://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/">blog posts</a> out there that provide intuition for PCA, but most tend to fall back on the idea of “maximizing variance explained”. This intuition never fully clicked for me. As my <a href="/blog/category/spatial-omics">research turned more toward graphs</a>, I tried to derive PCA from more of a graph perspective, omitting explicit mention of variance altogether. Here, I’ll briefly share the intuition that did end up clicking: it turns out that <strong>variance explained can instead be thought of as the similarity between two graphs</strong>.</p> <hr/> <h2 id="problem-statement">Problem statement</h2> <p>Consider the data matrix</p> \[\mathbf{X} \in \mathbb{R}^{n \times g},\] <p>where $n$ is the number of observations and $g$ is the number of features. Given my background in single-cell omics, I tend to think of this as a cell-by-gene matrix – hence the odd choice of the letter $g$. In this case, each row corresponds to a measured cell, each column the genes measured across all cells, and each entry the amount that a given gene is expressed in a given cell. (I’ll continue referring to observations as cells and features as genes just to tie things down to an application.) Viewed as a dimensionality reduction algorithm, the goal of PCA is to reduce “unnecessary” information, thereby emphasizing more prominent patterns in the data. But how do we specify that mathematically?</p> <p>First, we need to explicitly specify <strong>what patterns we’re interested in</strong>. The patterns we will focus on are the relationships between cells (although the relationships between genes would lead to the same result). More specifically, we will consider the pairwise relationships of all cells in terms of their genes. To do this for all pairs of cells, we can calculate the matrix product</p> \[\begin{equation} \label{eq:graphtrue} \mathbf{X} \mathbf{X}^{\top} \in \mathbb{R}^{n \times n}, \end{equation}\] <p>where each entry describes the similarity between cells $i$ and $j$ in terms of their inner product. Assuming proper mean centering of $\mathbf{X}$, this is simply a covariance matrix. However, in keeping with a graph perspective, we can instead think of this as a network connecting similar cells to one another. It’s not sparse like most adjacency matrices – in fact it’s dense with connections from each cell to all others. But these connections are weighted by similarity, enabling us to intuitively think of $\mathbf{X} \mathbf{X}^{\top}$ as a graph connecting molecularly similar cells.</p> <p>Next, we need to specify <strong>what “reducing information” means</strong>. Let’s think of it as trying to assign each cell a single value (i.e. a “metagene” describing multiple genes) while trying to maintain the original molecular relationships between cells. Quantitatively, these new values correspond to a vector $\mathbf{v} \in \mathbb{R}^n$. Just as the true molecular relationships between cells were given by $\mathbf{X} \mathbf{X}^{\top} \in \mathbb{R}^{n \times n}$, the graph resulting from our new values is given by</p> \[\begin{equation} \label{eq:graphpred} \mathbf{v} \mathbf{v}^{\top} \in \mathbb{R}^{n \times n}. \end{equation}\] <p>Finally, because we want to maintain the original graph structure as best we can, <strong>we need a way to compare the two graphs</strong> given by eqs. \eqref{eq:graphtrue} and \eqref{eq:graphpred}. We’ll do it by summing over the similarities between edge weights in each graph, which are just the individual entries. (To be clear, this is a bit meta and entails summing over “similarities between similarities”) This leads us to our final metric for the similarity between our two graphs:</p> \[\begin{equation} \label{eq:metric} \lambda = \sum_{ij} (\mathbf{X} \mathbf{X}^{\top})_{ij} (\mathbf{v} \mathbf{v}^{\top})_{ij}. \end{equation}\] <hr/> <h2 id="solution">Solution</h2> <p>Now we can ask: <strong>how do we maximize this similarity?</strong> It turns out we can solve this just by simplifying the equation. Let $\mathbf{C} = \mathbf{X} \mathbf{X}^{\top}$ represent our original graph of molecular relationships between cells. Then we can rearrange our metric into</p> \[\lambda = \sum_{ij} \mathbf{C}_{ij} v_i v_j.\] <p>This simplifies to the <a href="https://gregorygundersen.com/blog/2022/02/27/positive-definite/">quadratic form</a></p> \[\lambda = \mathbf{v}^{\top} \mathbf{C} \mathbf{v}.\] <p>Now consider that we don’t care about the overall magnitude of $\mathbf{v}$. Rather, we care about its <em>relative</em> differences among the cells. Thus, we should normalize our expression by dividing out the magnitude:</p> \[\lambda = \frac{\mathbf{v}^{\top} \mathbf{C} \mathbf{v}}{\mathbf{v}^{\top} \mathbf{v}}.\] <p>Finally, we can rearrange this equation into an eigenvalue problem:</p> \[\lambda = {\frac{\mathbf{v}^{\top} \mathbf{C} \mathbf{v}}{\mathbf{v}^{\top} \mathbf{v}}} \nonumber \\\] \[\Rightarrow \lambda \mathbf{v}^{\top} \mathbf{v} = \mathbf{v}^{\top} \mathbf{C} \mathbf{v} \nonumber \\\] \[\begin{equation} \label{eq:eigprob} \Rightarrow \lambda \mathbf{v} = \mathbf{C} \mathbf{v}. \end{equation}\] <p>Thus, the maximal $\mathbf{v}$ is just the eigenvector associated with the largest eigenvalue. In PCA, this eigenvector is PC1. The remaining eigenvectors, in order of eigenvalue, are PC2, PC3, etc. The loadings for each PC (i.e. how much each gene contributes to that pattern over the cell graph) can then be calculated by taking the inner product with each gene, i.e. $\mathbf{X}^{\top} \mathbf{v}$.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p><strong>In summary</strong>, we first represented the molecular relationships between cells as a graph with eq. \eqref{eq:graphtrue}. We then sought a reconstruction of that graph from less information, given by eq. \eqref{eq:graphpred}. To find the optimal values that do this, we calculated the similarity between the two graphs as eq. \eqref{eq:metric} and simplified it to eq. \eqref{eq:eigprob} to find that the top eigenvector of the original data graph provides the desired information.</p> <p>This may seem like quite a bit of legwork, but I think it’s worth it for a few reasons. First, it provides a purely geometric and visual derivation of PCA that may be <strong>more intuitive to those who feel comfortable with graphs</strong>. Second, it <strong>makes <a href="https://en.wikipedia.org/wiki/Kosambi%E2%80%93Karhunen%E2%80%93Lo%C3%A8ve_theorem">the connection between PCA and the Fourier transform</a> very clear</strong>, as <a href="https://arxiv.org/abs/2303.12211">eigenvectors of a symmetric matrix are just Fourier modes over the associated graph domain</a>. Thus, you can think of PCs as the lowest-frequency Fourier modes over the molecular similarity graph. (This is literally the same math introduced in my <a href="/blog/2025/graph-fourier">previous post on graph signal processing</a> except that it relies on an adjacency matrix rather than a Laplacian matrix).</p> <p>Hope this is insightful.</p>]]></content><author><name></name></author><category term="math"/><summary type="html"><![CDATA[An intuitive derivation of PCA based on graphs]]></summary></entry><entry><title type="html">Spatial Omics I: Transcriptional Signals Over Tissue Domains</title><link href="https://kmaherx.github.io/blog/2025/graph-fourier/" rel="alternate" type="text/html" title="Spatial Omics I: Transcriptional Signals Over Tissue Domains"/><published>2025-06-10T00:00:00+00:00</published><updated>2025-06-10T00:00:00+00:00</updated><id>https://kmaherx.github.io/blog/2025/graph-fourier</id><content type="html" xml:base="https://kmaherx.github.io/blog/2025/graph-fourier/"><![CDATA[<style>.slider-with-shadows{--default-handle-shadow:0 0 5px rgba(0,0,0,1);--divider-shadow:0 0 5px rgba(0,0,0,0.5)}</style> <h2 id="introduction">Introduction</h2> <p>Cells are arguably the fundamental unit of biology. They have different <a href="https://www.cell.com/cell-systems/fulltext/S2405-4712(18)30482-4">roles in the tissue that can largely be ascribed to their molecular contents</a>. Such “cell types” are often catalogued by efforts such as the <a href="https://www.humancellatlas.org/">Human Cell Atlas</a> which serve as valuable references for identifying <a href="https://www.cell.com/cell/fulltext/S0092-8674(17)30578-0">disease-associated cell types</a>. <strong>But cells don’t act independently.</strong> If you look at an organism, you’ll notice a few things. First, it’s an organism; there’s some sense that it’s a discrete collection of cells all working together. Second, you might notice that cells within a given organism are further organized into discrete multicellular structures, such as organs or the anatomical regions within them. Third, and perhaps most importantly, the very formation and function of such structures is driven by interactions between their constituent cells. Altogether, it is critical to look beyond isolated cells to recognize how they collectively govern tissue structure and function.</p> <p>We can break these patterns down into two broad categories.</p> <ol> <li><strong>Multicellular regions</strong> are what we’ll call molecularly-distinct collections of adjacent cells in physical space. We expect to observe these repeatedly within the same or across different tissues.</li> <li><strong>Intercellular interactions</strong> are what we’ll call the communication patterns between neighboring cells that likely define or even give rise to these regions.</li> </ol> <p>Both of these patterns are inherently spatial. However, it’s not clear what <strong>length scale</strong> they occur on: are we talking about pairs of cells or entire tissues? In this <a href="/blog/category/spatial-omics">series of posts</a>, we will leverage <a href="https://arxiv.org/abs/2303.12211">graph signal processing (GSP)</a> in <a href="https://www.nature.com/articles/s41587-022-01448-2">single-cell resolved spatial transcriptomics data</a> to isolate and investigate patterns on specific length scales. This will allow us to rigorously define regions and interactions in future posts. We’ll first demonstrate the fundamental concepts in a simple simulation. This will allow us to build up some intuition before approaching data gathered from real tissues, which are full of both technical artifacts and true biological complexity. It’ll also come in handy for future blog posts covering more complex ideas. The work in this series of posts is based on <a href="/assets/pdf/harmonics.pdf">this manuscript</a>.</p> <hr/> <h2 id="simulation">Simulation</h2> <p>To construct a simulated tissue, we’ll take inspiration from the mammalian brain, which has a <a href="https://www.nature.com/articles/s41586-021-03705-x">simple layered structure with clearly defined molecular markers for each layer region</a>. This will inform how we decide where the cells are (i.e. the <strong>domain</strong>) and what gene expression patterns they display (i.e. the <strong>signals</strong>).</p> <h3 id="the-tissue-domain">The tissue domain</h3> <p>We’ll start off by creating a simple tissue made up of $n=2000$ cells randomly scattered throughout a unit circle. Such a <a href="https://en.wikipedia.org/wiki/Poisson_point_process">Poisson point process</a> may not be a perfectly accurate reflection of the more structured physical distribution of cells in tissues, but <a href="https://www.nature.com/articles/s41592-025-02697-0">it is capable of yielding valuable insight nonetheless</a>. Furthermore, while tissues are inherently three-dimensional and might be more accurately simulated within a sphere, experimental technologies typically measure them in two-dimensional slices. Thus, we will rely on two-dimensional simulations. That being said, everything presented in this series of posts can be generalized to distributions of cells in three-dimensional space.</p> <p>Mathematically, we can think of the tissue domain as an undirected graph over $n$ nodes, each of which represents a cell. We could construct this graph in many ways, including connecting each cell to its $k$ nearest physical neighbors. However, I personally prefer using a Delaunay triangulation, as it creates a mesh that’s embeddable in 2D, which respects my own visual intuition. If you’re optimistic, you might also believe that it <a href="https://pubmed.ncbi.nlm.nih.gov/20082148/">captures the mechanical forces present in biological tissues</a>. After performing a Delaunay triangulation, we can zoom in to see that the cells are indeed connected to their spatial neighbors to form a 2D mesh. These connections define how we can hop from cell to cell. Altogether, this graph constitutes a domain: a set of positions and the movements we can make between them.</p> <figure style="text-align: center;"> <img src="/assets/figures/fourier/tissue_domain.png" alt="" style="width:50%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 1:</strong> Uniformly distributed points triangulated into a spatial graph to form a simulated tissue domain. </figcaption> </figure> <p>More explicitly, our graph can be represented by the symmetric adjacency matrix</p> \[\mathbf{A} \in \{0,1\}^{n \times n}.\] <p>Each entry \(\mathbf{A}_{ij}\) is either $1$, which represents two cells that are spatially adjacent, or $0$, which represents two cells that are not adjacent. While we could weight these edges based on physical distances between cells, we will instead stick to simple binary edges for simplicity. We also will not consider self loops: \(\mathbf{A}_{ii} = 0\). The number of neighbors for cell $i$ – the “degree” – is given by \(d_i = \sum_j \mathbf{A}_{ij}\). These values are often consolidated into the diagonal degree matrix $\mathbf{D} = \operatorname{diag}(d_1, …, d_n)$. Finally, we can introduce the Laplacian matrix</p> \[\mathbf{L} = \mathbf{D} - \mathbf{A}\] <p>We can think of the Laplacian as a slight modification of the adjacency matrix that’s more closely related to the notion of frequency, as we will see below. For convenience, we will instead consider the edge-normalized Laplacian $\mathbf{L} = \mathbf{I} - \mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}$, since it’s eigenvalues have some nice properties that we will leverage later.</p> <p>Note that the order of cells in these matrices is arbitrary. It doesn’t matter so long as it is consistent across all related vectors and matrices.</p> <h3 id="transcriptional-signals">Transcriptional signals</h3> <p>Now that we have a simulated tissue domain, we can create gene expression (i.e. transcriptional) signals over it. A signal is a vector of values associated with each node in our graph. In our case, we are interested in the amount that a given gene is expressed within each cell of a tissue. We can collect all of these values into a list, yielding the vector $\mathbf{x} \in \mathbb{R}^n$, where $\mathbf{x}_i$ is the amount of gene $x$ expressed in cell $i$. We can collect all of these gene signals into the matrix</p> \[\mathbf{X} = [\mathbf{x}_1 | ... | \mathbf{x}_g] \in \mathbb{R}^{n \times g},\] <p>where $g$ is the number of genes measured. This is the “cell-by-gene matrix”, the <a href="https://cellxgene.cziscience.com/">fundamental data structure underlying all of single-cell omics</a>.</p> <p>Let’s add some signals to our simulation. The first type of spatial pattern we are interested in is <strong>multicellular regions</strong>. In the neocortex, <a href="https://atlas.brain-map.org/atlas?atlas=1&amp;plate=100960348">gene expression patterns define distinct layers</a>. To simulate such patterns, we can first assign ground truth region labels to all cells in our tissue. We’ll do so by splitting the tissue into four layers radially, creating a bullseye pattern. However, in the brain, these layer-specific genes are mainly expressed by only a subset of cells – specifically neurons. To reflect this, we will take a subset of the cells within each ground truth region and assign them a ground truth “spatial” cell type identity. The rest of the cells will be grouped together and randomly assigned a “non-spatial” type, reminiscent of glial (i.e. non-neuronal) cells. Finally, each type is assigned multiple gene markers by adding different instances of uniform noise to its corresponding ground truth pattern (though only one gene marker is shown per type in the figure below).</p> <figure style="text-align: center;"> <img src="/assets/figures/fourier/simulation_lows.png" alt="" style="width:100%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 2:</strong> Schematic for simulating multicellular region patterns and their associated cell types and gene markers. </figcaption> </figure> <p>The second type of spatial pattern we are interested in is <strong>intercellular interactions</strong>. Such interactions can be mediated by many molecular mechanisms, but one simple one that we will simulate here is <a href="https://www.ncbi.nlm.nih.gov/books/NBK10072/">juxtacrine signaling</a>, in which cells communicate via direct contact. For instance, a juxtacrine interaction may occur between two cells when one cell expresses a ligand, the other cell expresses the corresponding receptor, and the two cells are adjacent to one another. We’ll refer to one of these cells as the “sender” and the other the “receiver”. We can simulate this pattern by randomly assigning a sparse set of cells in the tissue a ground truth sender type and then assigning all of their neighbors a ground truth receiver type. If the ground truth sender signal is given by \(\mathbf{s} \in \{0,1\}^n\), we can spread it to its neighbors by multiplying with the adjacency matrix, yielding the ground truth receiver signal $\mathbf{r} = \mathbf{A} \mathbf{s}$. (For consistency, we can ensure that \(\mathbf{r} \in \{0,1\}^n\) by thresholding to assign unit value to anything nonzero.) Just as above, we can then add uniform noise to create gene markers.</p> <figure style="text-align: center;"> <img src="/assets/figures/fourier/simulation_highs.png" alt="" style="width:100%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 3:</strong> Schematic for simulating intercellular interaction patterns and their associated cell types and gene markers. </figcaption> </figure> <p>Together, our Laplacian and cell-by-gene matrices $\mathbf{L}$ and $\mathbf{X}$ describe a domain and a set of signals with which we can perform signal processing to identify patterns on specific length scales. </p> <h3 id="frequencies">Frequencies</h3> <p>The first step in isolating a given length scale is to ask: <strong>how do we define length scales?</strong> In the language of signal processing, we define them in terms of <strong>frequencies</strong>. Take music, for instance, in which the different frequency ranges of bass, mids, and trebles define different <em>time</em> scales along which a sound can vary. Similarly, our notion of <em>length</em> scale can be made quantitatively rigorous in terms of <em>spatial</em> frequencies. While a straightforward extension of this concept to two dimensions captures spatial frequencies in an image, it’s not so straightforward to extend to graphs due to their irregular topology; an image is a uniform grid of points, whereas a graph can have arbitrary connections between them. Thus, we have to do a little leg work to extend this concept to our tissue domain.</p> <p>We can start with the intuitive definition of frequency in a graph setting. Consider a hypothetical signal $\mathbf{v}$ over the tissue. The notion of frequency is simply “how much does the signal tend to change as I take a step in the domain?” Taking a step in our tissue domain corresponds to moving between neighboring cells $i$ and $j$. The change in our signal as we take this step is thus given by $\mathbf{v}_i - \mathbf{v}_j$. However, we don’t really care about the sign, so we can just square it to get $(\mathbf{v}_i - \mathbf{v}_j)^2$. This is just one step, and we care about how the signal tends to change with steps throughout the whole tissue in general. To capture this tendency, we can simply sum over all pairs of neighboring cells, yielding our final definition of frequency</p> <p>\begin{equation} \label{eq:freqdef} \lambda = \sum_{ij} \mathbf{A}_{ij} (\mathbf{v}_i - \mathbf{v}_j)^2 \end{equation}</p> <p>Note that because we are summing over <em>all</em> possible pairs of cells $i$ and $j$, we have to multiply each one by $\mathbf{A}_{ij}$ so that we only consider <em>neighboring</em> pairs.</p> <p>This definition of frequency for a given signal might make sense, but what we are really looking for is an ideal <em>set</em> of signals that represent <em>all possible frequencies</em>, much as time scales are given by sine waves of all possible frequencies. It turns out that, because our tissue domain is finite and discrete, we can actually solve for a finite set of all possible frequencies, and it ends up being a basis in the linear algebraic sense.</p> <p>To see this, let’s first rewrite eq. \eqref{eq:freqdef} as a “<a href="https://gregorygundersen.com/blog/2022/02/27/positive-definite/">quadratic form</a>”, which works in our favor by getting us a step further into the realm of linear algebra:</p> \[\lambda = \mathbf{v}^{\top} \mathbf{L} \mathbf{v}.\] <details><summary>How exactly can we show that?</summary> <p>Eq. \eqref{eq:freqdef} can be converted into a quadratic form via a process reminiscent of annealing. First, we heat it up by factoring the $\mathbf{v}$ terms to get</p> \[\sum_{i,j}\mathbf{A}_{ij}( v_i v_i + v_j v_j - v_i v_j - v_j v_i ).\] <p>We can then begin to cool it down by combining positive and negative terms, respectively, yielding</p> \[2 \sum_{i,j}\mathbf{A}_{ij} v_i v_i - 2 \sum_{i,j}\mathbf{A}_{ij} v_i v_j.\] <p>Note that, for a fixed row $i$, the left-hand term corresponds to summing $v_i v_i$ together $d_i$ times, where $d_i$ is the sum of the $i$th row of $\mathbf{A}$. Thus, one could equivalently express the left-hand term as</p> \[2 \sum_{i}\mathbf{D}_{ii} v_i v_i.\] <p>Now both the left and right terms themselves correspond to quadratic forms, yielding</p> \[2 \mathbf{v}^{\top} \mathbf{D} \mathbf{v} - 2 \mathbf{v}^{\top} \mathbf{A} \mathbf{v}.\] <p>Further simplifying, we have</p> \[2 \mathbf{v}^{\top} (\mathbf{D}-\mathbf{A}) \mathbf{v}\] <p>and further</p> \[2 \mathbf{v}^{\top} \mathbf{L} \mathbf{v}.\] <p>The factor of two arises from double counting each edge, which corresponds to a directed edge in each direction. Because we tend to think of each undirected edge as only a single edge going both ways, folks tend to omit this factor, yielding the final expression</p> \[\mathbf{v}^{\top} \mathbf{L} \mathbf{v}.\] </details> <p>We can further rearrange this expression into an eigenvalue problem:</p> \[\lambda \mathbf{v} = \mathbf{L} \mathbf{v}.\] <details><summary>How exactly can we show that?</summary> <p>We should probably normalize $\lambda = \mathbf{v}^{\top} \mathbf{L} \mathbf{v}$ to guarantee that the overall expression strength doesn’t influence the frequency value. After all, we really only care about the relative spatial distribution of the signal, not its overall magnitude. We can do this normalization by dividing the magnitude out:</p> \[\lambda = \frac{\mathbf{v}^{\top} \mathbf{L} \mathbf{v}}{\mathbf{v}^{\top} \mathbf{v}}.\] <p>Note that this is a Rayleigh quotient, which is intimately tied to eigendecomposition. With a bit of rearranging, we end up with an eigenvalue problem:</p> \[\begin{align} &amp; \lambda = {\frac{\mathbf{v}^{\top} \mathbf{L} \mathbf{v}}{\mathbf{v}^{\top} \mathbf{v}}} \nonumber \\ &amp;\rightarrow \lambda \mathbf{v}^{\top} \mathbf{v} = \mathbf{v}^{\top} \mathbf{L} \mathbf{v} \nonumber \\ &amp;\rightarrow \lambda \mathbf{v} = \mathbf{L} \mathbf{v}. \nonumber \end{align}\] </details> <p>This is a critical insight because $\mathbf{L}$ is symmetric positive semidefinite (PSD) and thus has an eigenbasis of eigenvectors with real, nonnegative eigenvalues. This eigenbasis is given by the matrix</p> \[\mathbf{V} = [\mathbf{v}_1 |...| \mathbf{v}_n] \in \mathbb{R}^{n \times n}.\] <p>Let’s plug all of this terminology into our context of interest by realizing two key points. <strong>First</strong> of all, we found that there exists a finite set of ideal signals ${\mathbf{v}_1, …, \mathbf{v}_n}$ that represent “all possible frequencies” – and thus all possible length scales – over our tissue domain. This is great because it means we can rigorously talk about any length scale of interest. Below are some examples plotted in the tissue to illustrate this intuition. Note that no gene expression information went into calculating these frequencies; rather, they are an abstract representation of variation on different length scales.</p> <figure style="text-align: center;"> <img src="/assets/figures/fourier/frequencies.png" alt="" style="width:100%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 4:</strong> Examples of different frequencies over the tissue domain.</figcaption> </figure> <details><summary>Why do highs appear constrained to one part of the tissue?</summary> <p>When taking a look at $\mathbf{v}_{301}$, the fluctuations appear constrained a bit toward the lower left of the tissue. I’ll just provide some intuition for this observation.</p> <p>This asymmetry (or “localization”) is due to the irregularity of the graph domain. Consider the time and image domains in which you always have the same choice of how to move no matter where you are in the tissue (apart from the boundaries). Graphs generally lack this regularity due to the variation in degree across different nodes. At one node, you may have four options of where to move next. At the next node, you may have seven options.</p> <p>Consequences of this irregularity arise when considering the maximum frequency patterns over the graph, which we can think of as</p> \[\mathbf{v}_{max} = \operatorname{argmax}_{\mathbf{v}} \frac{\mathbf{v}^{\top} \mathbf{L} \mathbf{v}}{\mathbf{v}^{\top} \mathbf{v}}\] <p>Because of its normalization, \(\mathbf{v}_{max}\) should ideally be concentrated on a single node and its immediate neighbors. Furthermore, it should specifically be concentrated on the node with the most neighbors, i.e. the highest degree. Indeed, when visualizing $\mathbf{v}_{1994}$, we can see that it’s concentrated on nodes with relatively high degrees (compared to the <a href="https://en.wikipedia.org/wiki/Delaunay_triangulation#Properties">degree of six we’d expect here</a>).</p> <figure style="text-align: center;"> <img src="/assets/figures/fourier/freq_localization.png" alt="" style="width:70%; display: block; margin: 0 auto;"/> </figure> <p>Interestingly, this phenomenon has deep connections to the <a href="https://arxiv.org/abs/2306.15810">Heisenberg uncertainty principle</a>.</p> </details> <p>The <strong>second</strong> key point is that this set of ideal frequencies forms a basis in the linear algebraic sense. This means that when we do measure a gene expression signal, we can project it into this “frequency space” to quantify its prevalence on each length scale.</p> <p>That’s what we’ll do next.</p> <h3 id="spectra">Spectra</h3> <p>We can think of projecting a given gene expression signal $\mathbf{x}$ into frequency space as comparing it to each frequency $\mathbf{v}_i$. For a single frequency, this is given by the inner product $\mathbf{v}_i^{\top} \mathbf{x} \in \mathbb{R}$. For all frequencies, this is given by the matrix product $\mathbf{V}^{\top} \mathbf{x} \in \mathbb{R}^n$, where the output is the similarity of $\mathbf{x}$ to each of $\mathbf{v}_1, …, \mathbf{v}_n$. This vector is often referred to as the signal’s “spectrum”. An analogy I tend to think of is that the original gene expression signal over the tissue is like a dish you might cook. You could always think of that dish in terms of its corresponding <em>recipe</em>, i.e. the amounts of each ingredient necessary to construct it. In this case, the recipe is the spectrum, and the ingredients are the frequencies. This process of representing a signal in terms of its spectrum is known as the <a href="https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/">Fourier transform</a>.</p> <p>Let’s visualize one of our region marker genes’ spectra as an example.</p> <figure style="text-align: center;"> <img src="/assets/figures/fourier/spectra.png" alt="" style="width:100%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 5:</strong> An example gene expression signal in tissue space and in frequency space. </figcaption> </figure> <p>By design, this gene forms a large-scale pattern over the tissue. As a result, its spectrum shows a spike in the low frequencies. Thus, we now have a way of calculating the frequency contents (or length scale contents) of a given gene expression signal within a tissue.</p> <p>Note that, while the spectrum shown above is entirely positive, spectra generally do contain negative values. We just chose to take the absolute value of the spectrum to better convey the intuition of how prevalent a signal is over a given length scale, i.e. omitting its sign. Additionally, we chose not to visualize the first value, as it just corresponds a translation factor.</p> <details><summary>Why is the first value just a translation factor?</summary> <p>What I mean by a “translation factor” here is just an “intercept” or “bias” term that is added to all cells to shift their expression values up/down by the same amount. Thus, we can think of it as some vector with entries that are all the same value, i.e. a scaled version of the ones vector $\mathbf{1} = [1, …, 1]^{\top}$. We are assuming that it corresponds to the first eigenvalue of $\mathbf{L}$, so let’s first confirm that it’s an eigenvector by multiplying them. Notice that</p> \[\mathbf{L} \mathbf{1} = 0.\] <p>This follows from how we’ve constructed the Laplacian; the degree of each node is on the diagonal and the instances of each neighbor on the off diagonal sum up to an amount of equal but opposite sign. Thus, the sum over each row, as calculated above by multiplying with the ones vector, is simply $0$. We can write this out in the form of an eigenvalue problem:</p> \[\mathbf{L} \mathbf{1} = \lambda \mathbf{1} = 0.\] <p>This holds true for $\lambda = 0$, which is the minimal eigenvalue (i.e. “first” when sorted) since $\mathbf{L}$ is PSD. So the first eigenvalue corresponds to the all ones vector, i.e. the uniform shift in all the values over the graph that we wanted to get rid of.</p> <p>Note that this generalizes to any vector with all equal entries because we could just express it as the ones vector scaled by some scalar $p$, which yields the same result:</p> \[p \mathbf{L} \mathbf{1} = p \lambda \mathbf{1} = 0.\] </details> <p>There are many interesting things we could do with spectra. You might think of comparing gene expression signals based on their spectra, perhaps to find classes of spectral patterns (i.e. biologically-relevant length scales) using PCA or NMF. However, there are also many interesting issues with these ideas. For instance, a spectrum has the same dimension as its corresponding tissue. This makes comparison across different tissues difficult because the dimensions of spectra likely differ. We’ll leave these pitfalls and opportunities to a future blog post.</p> <p>For now, let’s instead focus on modifying these spectra, i.e. performing filtering.</p> <h3 id="filtering">Filtering</h3> <p>In the language of the above analogy, modifying spectra allows us to ask “what happens to the dish when I remove this ingredient?” More explicitly, it allows us to modify the frequency contents of a given signal and see how it looks in the tissue. We can think of this process as two steps.</p> <p>The first step is to <strong>modify the signal’s spectrum</strong>. We could do this in a crude way by setting entries of the spectrum to zero to get rid of them entirely. However, more generally, we could <em>weight</em> them. For instance, we could define some function – or “kernel” – over frequency values that preferentially weights lows stronger than highs. This is known as a “low-pass” kernel. One popular example of such a function is the diffusion kernel $f(\lambda) = e^{-\tau \lambda}$, where $\tau$ is some parameter that adjusts how “quick” diffusion occurs. This kernel is indeed low-pass, as it is a monotonically decreasing function of frequency. (The intuition of diffusion will likely become clearer once we see the result of this modification in the tissue.) By pointwise multiplying this kernel with the gene expression spectrum from earlier, we end up with a modified spectrum where the lows are maintained and the highs are diminished.</p> <figure style="text-align: center;"> <img src="/assets/figures/fourier/lowpass_spectra.png" alt="" style="width:100%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 6:</strong> Low-pass filtering of a gene expression signal in frequency space. </figcaption> </figure> <p>Now let’s represent this mathematically. Pointwise multiplication of two vectors can be represented by turning one of them into a diagonal matrix and then multiplying. Let’s do this with the kernel. We can first arrange all of the frequency values into a diagonal matrix $\mathbf{\Lambda}$. Applying the kernel function to each of the values is the same as applying it to the whole matrix, i.e. $f(\mathbf{\Lambda})$. Multiplication of this diagonal kernel matrix with the spectrum is then given by $f(\mathbf{\Lambda}) \mathbf{V}^{\top} \mathbf{x} \in \mathbb{R}^n$. Altogether, this equation describes modification of the gene’s spectrum. While it might look a little ugly in its current form, it’s nice to keep it this way for later; it’ll allow us to see a neat simplification in a moment.</p> <p>The second step is to <strong>project the modified spectrum back into the tissue</strong> to visualize the result. We can do this by multiplying by the inverse of the frequency basis, i.e. $(\mathbf{V}^{\top})^{-1}$. However, because $\mathbf{V}$ is an <a href="https://gregorygundersen.com/blog/2018/10/24/matrices/">orthogonal matrix</a>, its inverse is just its transpose: $(\mathbf{V}^{\top})^{-1} = \mathbf{V}$. Thus, we have the full filtering equation</p> <p>\begin{equation} \label{eq:filterdef} \mathbf{\bar x} = \mathbf{V} f(\mathbf{\Lambda}) \mathbf{V}^{\top} \mathbf{x}, \end{equation}</p> <p>where $\mathbf{\bar x}$ is the filtered version of $\mathbf{x}$. I like to use the bar notation for the smoothed signal because the bar itself is smooth. We can visualize the result of this process below. Use the slider to visualize the signal before filtering (left) and after filtering (right).</p> <div style="width: 50%; max-width: 768px; margin: 0 auto;"> <img-comparison-slider class="slider-with-shadows"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/fourier/tissue_before_filtering-480.webp 480w,/assets/figures/fourier/tissue_before_filtering-800.webp 800w,/assets/figures/fourier/tissue_before_filtering-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/fourier/tissue_before_filtering.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/fourier/tissue_after_lowpass-480.webp 480w,/assets/figures/fourier/tissue_after_lowpass-800.webp 800w,/assets/figures/fourier/tissue_after_lowpass-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/fourier/tissue_after_lowpass.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> </div> <figcaption><strong>Figure 7:</strong> Comparison of a gene expression signal before (left image) and after (right image) low-pass filtering. </figcaption> <p><br/></p> <p>This filter appears to have blurred the underlying gene expression signal. That’s exactly what we expect given that blurring corresponds to getting rid of small-scale fluctuations. Some might think of this as <a href="https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-024-01283-x">denoising</a>, which makes sense in the context of images. However, in a later post I’ll argue that high frequencies are not noise in the context of tissues.</p> <p>Using a final bit of math, let’s finish interpreting our filtering function, which is currently a bit lengthy. If you stare at eq. \eqref{eq:filterdef} long enough, you might notice that it looks a lot like the <a href="https://intuitive-math.club/linear-algebra/eigenbasis/">diagonalized</a> form of the Laplacian</p> \[\mathbf{L} = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^{\top},\] <p>where $\mathbf{\Lambda}$ is the diagonal matrix of eigenvalues and $\mathbf{V}$ is the matrix of corresponding eigenvectors. The only difference is that we applied a function to each of the eigenvalues, i.e. $f(\mathbf{\Lambda})$. Interestingly, for any (analytic) function $h(\lambda)$, we have</p> \[\mathbf{V} h(\mathbf{\Lambda}) \mathbf{V}^{\top} = h(\mathbf{V} \mathbf{\Lambda} \mathbf{V}^{\top}) = h(\mathbf{L}).\] <details><summary>Why “analytic” functions?</summary> <p>Functions of matrices are often defined in terms of series, <a href="https://sassafras13.github.io/MatrixExps/">e.g. the exponential function</a>. A function that can be described in this way is referred to as “<a href="https://en.wikipedia.org/wiki/Analytic_function#Definitions">analytic</a>”. Thus, for our kernel function to apply to matrix arguments, it must be analytic.</p> </details> <p>Thus, any (analytic) filter can be expressed as a function of the Laplacian. This is cool for a few reasons. First of all, it’s pretty. Second, it will actually help us interpret equations that emerge en route to deriving interactions in a future post. Third, it gives us a concise notation to work with for the remainder of this post; given a kernel such as the one above, $f$, filtering can be expressed simply as</p> \[\mathbf{\bar x} = f(\mathbf{L}) \mathbf{x}.\] <p>While the above filtering example was low-pass, we could instead perform high-pass filtering. First, let’s define a kernel that preferentially weights highs. We can use the square-root kernel $g(\lambda) = \lambda^{\frac{1}{2}}$, as it is monotonically increasing with frequency. (This will be an important kernel for defining interactions in a later post.) Using the concise notation from above (along with a hat to convey the “opposite” of smoothness), we can apply this filter to calculate the high-pass filtered signal</p> \[\mathbf{\hat x} = g(\mathbf{L}) \mathbf{x}.\] <p>In contrast to low-pass filtering, we can see in the modified spectrum that the high-pass kernel emphasizes the higher frequency components of our signal.</p> <figure style="text-align: center;"> <img src="/assets/figures/fourier/highpass_spectra.png" alt="" style="width:100%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 8:</strong> High-pass filtering of a gene expression signal in frequency space. </figcaption> </figure> <p>When visualized in the tissue, it appears that the “roughness” of our signal is preserved, and everything else is washed away. Cells with large differences with their neighbors in the original signal maintain their differences after filtering, ending up with extremal values. On the other hand, cells with little differences with their neighbors end up with values in the middle after filtering. Thus, <em>high</em>-pass filtering appears to emphasize <em>differences</em> in gene expression between neighboring cells, unlike the <em>similarities</em> highlighted by <em>low</em>-pass filtering.</p> <div style="width: 50%; max-width: 768px; margin: 0 auto;"> <img-comparison-slider class="slider-with-shadows"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/fourier/tissue_before_filtering-480.webp 480w,/assets/figures/fourier/tissue_before_filtering-800.webp 800w,/assets/figures/fourier/tissue_before_filtering-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/fourier/tissue_before_filtering.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/fourier/tissue_after_highpass-480.webp 480w,/assets/figures/fourier/tissue_after_highpass-800.webp 800w,/assets/figures/fourier/tissue_after_highpass-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/fourier/tissue_after_highpass.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> </div> <figcaption><strong>Figure 9:</strong> Comparison of a gene expression signal before (left image) and after (right image) high-pass filtering. </figcaption> <p><br/></p> <p>Low- and high-pass filters are just two examples at the extremes of the frequency range. Many other filter shapes have interesting and conceptually interpretable behaviors, such as mid-pass filters and those that are highly localized in the frequency domain. However, for simplicity, we won’t go into those here.</p> <p>Despite our analytical treatment of filtering, explicit eigendecomposition of the Laplacian is prohibitive for tissues (graphs) with greater than approximately $n=20000$ cells (nodes). For that reason, filtering is often calculated using <a href="https://arxiv.org/abs/0912.3848">wavelet approximations</a>. The <em>de facto</em> package for performing this analysis is <a href="https://pygsp.readthedocs.io/en/stable/">PyGSP</a>, and that’s what we will use for all real biological datasets going forward.</p> <hr/> <h2 id="mouse-brain">Mouse brain</h2> <p>Now, with the intuition gained from our simulation, we will pivot to <a href="https://www.nature.com/articles/s41586-021-03705-x">real data from the mouse brain</a>. This dataset is composed of 64 samples from the mouse primary motor cortex (MOp), which displays the molecularly-defined layered structure that inspired our simulation. Each sample was collected using <a href="https://www.biorxiv.org/content/10.1101/2023.12.07.570603v2">MERFISH</a> and consists of 248 genes measured over ~5000 cells. We will start by visualizing the results from one sample before generalizing our analysis to multiple.</p> <h3 id="one-sample">One sample</h3> <p>Just as in our simulation, we can create a spatial mesh over the tissue to create a domain represented by the Laplacian $\mathbf{L}$.</p> <figure style="text-align: center;"> <img src="/assets/figures/fourier/tissue_domain_mop.png" alt="" style="width:70%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 10:</strong> A tissue domain calculated from real mouse brain data. </figcaption> </figure> <p>Once we have that domain, we can identify frequency patterns (length scales) over it by calculating the eigenbasis $\mathbf{V}$ of the Laplacian. Again, we see that these eigenvectors and values capture different length scales of variation over the tissue.</p> <figure style="text-align: center;"> <img src="/assets/figures/fourier/frequencies_mop.png" alt="" style="width:100%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 11:</strong> Example frequencies over the mouse brain tissue domain. </figcaption> </figure> <p>We can then project a given gene expression pattern $\mathbf{x}$ into frequency space by calculating $\mathbf{V}^{\top} \mathbf{x}$. Let’s do this with the gene <em>Cux2</em>, a neocortical layer marker analogous to the simulated gene we visualized above. The spectrum conveys the prominent large-scale pattern in terms of a low-frequency spike.</p> <figure style="text-align: center;"> <img src="/assets/figures/fourier/spectra_mop.png" alt="" style="width:100%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 12:</strong> An example gene expression signal in tissue space and in frequency space. </figcaption> </figure> <p>Given the diffusion kernel $f(\lambda) = e^{-\tau \lambda}$, we can perform low-pass filtering using the equation $f(\mathbf{L}) \mathbf{x}$. This smooths the original signal, in effect isolating the component of the signal that occurs on large length scales.</p> <div style="width: 50%; max-width: 768px; margin: 0 auto;"> <img-comparison-slider class="slider-with-shadows"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/fourier/tissue_before_filtering_mop-480.webp 480w,/assets/figures/fourier/tissue_before_filtering_mop-800.webp 800w,/assets/figures/fourier/tissue_before_filtering_mop-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/fourier/tissue_before_filtering_mop.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/fourier/tissue_after_lowpass_mop-480.webp 480w,/assets/figures/fourier/tissue_after_lowpass_mop-800.webp 800w,/assets/figures/fourier/tissue_after_lowpass_mop-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/fourier/tissue_after_lowpass_mop.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> </div> <figcaption><strong>Figure 13:</strong> Comparison of a gene expression signal before (left image) and after (right image) low-pass filtering. </figcaption> <p><br/></p> <p>We could instead high-pass filter to isolate the small-scale components by calculating $g(\mathbf{L}) \mathbf{x}$ where $g(\lambda) = \lambda^{\frac{1}{2}}$. This emphasizes local variations in the signal by assigning them extreme values while washing away the rest of the signal toward middling values.</p> <div style="width: 50%; max-width: 768px; margin: 0 auto;"> <img-comparison-slider class="slider-with-shadows"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/fourier/tissue_before_filtering_mop-480.webp 480w,/assets/figures/fourier/tissue_before_filtering_mop-800.webp 800w,/assets/figures/fourier/tissue_before_filtering_mop-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/fourier/tissue_before_filtering_mop.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/fourier/tissue_after_highpass_mop-480.webp 480w,/assets/figures/fourier/tissue_after_highpass_mop-800.webp 800w,/assets/figures/fourier/tissue_after_highpass_mop-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/fourier/tissue_after_highpass_mop.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> </div> <figcaption><strong>Figure 14:</strong> Comparison of a gene expression signal before (left image) and after (right image) high-pass filtering. </figcaption> <p><br/></p> <p>But there are 63 other samples in the full dataset. How might we analyze all of them together?</p> <h3 id="multiple-samples">Multiple samples</h3> <p>A conceptual issue comes up when considering multiple samples. First, note that different slices entail different domains because they are made up of different sets of cells. Now consider two slices with domains given by Laplacians $\mathbf{L}^{(1)} \in \mathbb{R}^{n_1 \times n_1}$ and $\mathbf{L}^{(2)} \in \mathbb{R}^{n_2 \times n_2}$. These two slices likely have different numbers of cells, i.e. $n_1 \neq n_2$. Thus, the dimensions of these two matrices are likely not even the same. Given that a filter is a function of the Laplacian, <strong>one might wonder if filter kernels have consistent behavior across different tissues</strong>.</p> <p>A key observation, however, is that filters are really functions of the <em>eigenvalues</em> of the Laplacian. So if the eigenvalues of different Laplacians are somehow comparable, then the filters should be as well. These eigenvalues are determined by the graph’s topology, i.e. structural features such as the distribution of node degrees. For two reasons, we argue that all tissue domains we construct have similar topologies and thus similar filter behavior.</p> <ol> <li>Edge-normalization of the Laplacian matrix yields eigenvalues in the range $[0,2]$, yielding consistent frequency ranges across different graphs.</li> <li>We assume that cells within a tissue generally resemble a uniform discrete sampling of continuous 2D space. In that case, given the consistent Delaunay triangulation approach, <a href="https://arxiv.org/abs/1907.12972">filtering behavior converges in the limit of sampling size</a>.</li> </ol> <p>Given these assumptions, we should be able to simply apply a given filter to each sample independently to yield comparable results.</p> <p>Let’s take exactly that approach with low-pass filtering for illustration. We applied the same filter kernel to the Laplacian and gene signal for each sample:</p> \[\mathbf{\bar x^{(1)}}, ..., \mathbf{\bar x^{(64)}} = f(\mathbf{L}^{(1)}) \mathbf{x^{(1)}}, ..., f(\mathbf{L}^{(64)}) \mathbf{x^{(64)}}.\] <p>The resulting smoothed patterns appear exactly as we saw for a single sample above, indicating that we can indeed expect comparable filtering results across different tissues.</p> <div style="width: 100%; max-width: 768px; margin: 0 auto;"> <img-comparison-slider class="slider-with-shadows"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/fourier/tissue_before_filtering_mop_all-480.webp 480w,/assets/figures/fourier/tissue_before_filtering_mop_all-800.webp 800w,/assets/figures/fourier/tissue_before_filtering_mop_all-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/fourier/tissue_before_filtering_mop_all.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/fourier/tissue_after_lowpass_mop_all-480.webp 480w,/assets/figures/fourier/tissue_after_lowpass_mop_all-800.webp 800w,/assets/figures/fourier/tissue_after_lowpass_mop_all-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/fourier/tissue_after_lowpass_mop_all.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> </div> <figcaption><strong>Figure 15:</strong> Comparison of a gene expression signal before (left image) and after (right image) low-pass filtering. </figcaption> <p><br/></p> <p>Altogether, the same approach derived from our simulation enables us to characterize gene expression along specific length scales in real biological data. Note that this process is the same as conventional single-cell analysis across multiple datasets with the addition of a preliminary spatial filtering step.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>We set out with the goal of describing gene expression signals on arbitrary length scales, which is the first step to identifying multicellular regions and intercellular interactions. We ended up showing that signal processing provides a rigorous framework for doing so. Now that we’ve developed this framework for individual gene signals, the next step is to generalize it to <strong>multiple genes at a time</strong> to characterize their relationships on a given length scale. It might be intuitive that this could help us represent multicellular regions, for instance. For instance, we should be able to low-pass filter gene expression patterns and then plug them into the standard single-cell workflow to identify large-scale clusters, i.e. regions. Indeed, this is the cornerstone of all region identification methods in the field, from those based on <a href="https://www.nature.com/articles/s41592-022-01657-2">simple spatial smoothing</a> to those based on <a href="https://www.nature.com/articles/s41467-023-36796-3">complex graph neural networks</a>.</p> <p>In <a href="/blog/category/spatial-omics">the following posts</a>, <strong>we will leverage the tools we established here to establish a conceptually and quantitatively consistent framework for defining regions and interactions</strong>. In particular, we will find that <em>positively</em> covarying <em>low</em>-frequency patterns define <a href="/blog/2025/regions">regions</a> while <em>negatively</em> covarying <em>high</em>-frequency patterns define interactions.</p>]]></content><author><name>Kamal Maher</name></author><category term="spatial-omics"/><summary type="html"><![CDATA[An introduction to graph signal processing in spatial omics data]]></summary></entry><entry><title type="html">Spatial Omics II: Defining Multicellular Regions</title><link href="https://kmaherx.github.io/blog/2025/regions/" rel="alternate" type="text/html" title="Spatial Omics II: Defining Multicellular Regions"/><published>2025-06-10T00:00:00+00:00</published><updated>2025-06-10T00:00:00+00:00</updated><id>https://kmaherx.github.io/blog/2025/regions</id><content type="html" xml:base="https://kmaherx.github.io/blog/2025/regions/"><![CDATA[<style>.slider-with-shadows{--default-handle-shadow:0 0 5px rgba(0,0,0,1);--divider-shadow:0 0 5px rgba(0,0,0,0.5)}</style> <h2 id="introduction">Introduction</h2> <p>In this post, we’ll follow up on our <a href="/blog/2025/graph-fourier">previous work</a> to identify regions in spatial data. By performing PCA on low-pass filtered gene expression signals, we can gain insight into the anatomical structure of a tissue.</p> <p>However, this is the essence of <a href="https://pachterlab.github.io/LP_2021/current-analysis.html#region">so many other methods</a>. So we should ask the question: <strong>why would we do all this work just to replicate what so many other methods already do?</strong> The answer is that it will build a bridge between existing conceptual and quantitative notions, <strong>providing a theoretical foundation</strong> for spatial omics. Upon this foundation, we will end up constructing additional representations of features such as interactions that other methods struggle with. Furthermore, these representations will reside <strong>entirely on the gene level</strong> and will enable <strong>fully unsupervised</strong> analysis, whereas existing methods <a href="https://squidpy.readthedocs.io/en/stable/notebooks/examples/graph/compute_interaction_matrix.html">rely on cell types</a> (which obscure the rich gene level information) and/or <a href="https://www.nature.com/articles/s41467-021-21246-9">databases of known interactions</a>.</p> <p>But that’s a lot of grand ideas that will come up later. For now, let’s just have fun thinking about this typical approach in a quantitatively rigorous way.</p> <hr/> <h2 id="simulation">Simulation</h2> <p>Recall that <strong>low-pass filtering isolates large-scale patterns.</strong> Let’s visualize the result of low-pass filtering on a given gene expression signal over the tissue. Consider a marker gene for one of the inner regions of <a href="/blog/2025/graph-fourier#simulation">the simulated tissue that we constructed previously</a>. Drag the slider left to right to compare the signal before and after filtering.</p> <div style="width: 50%; max-width: 768px; margin: 0 auto;"> <img-comparison-slider class="slider-with-shadows"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/fourier/tissue_before_filtering-480.webp 480w,/assets/figures/fourier/tissue_before_filtering-800.webp 800w,/assets/figures/fourier/tissue_before_filtering-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/fourier/tissue_before_filtering.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/fourier/tissue_after_lowpass-480.webp 480w,/assets/figures/fourier/tissue_after_lowpass-800.webp 800w,/assets/figures/fourier/tissue_after_lowpass-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/fourier/tissue_after_lowpass.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> </div> <figcaption><strong>Figure 1:</strong> Comparison of a gene expression signal before (left image) and after (right image) low-pass filtering. </figcaption> <p><br/></p> <p>Note that the small-scale variation in expression has been washed away. Neighboring cells are forced to look more similar to one another, in effect isolating only <em>large</em>-scale patterns. With these low-pass filtered signals, we can begin <strong>comparing them to find interesting gene-gene relationships</strong> that ultimately describe regions.</p> <h3 id="simulated-region-components">Simulated region components</h3> <p>Intuitively, a <strong>group of gene expression patterns that overlap in the tissue should represent a region.</strong> We can find these groups by looking at pairwise relationships between gene signals. Consider low-pass filtered gene signals $\mathbf{\bar x}_i, \mathbf{\bar x}_j \in \mathbb{R}^n$. Because these signals are vectors and we want a scalar measure of similarity. One way is to compare them by taking the inner product:</p> \[\mathbf{\bar x}_i^{\top} \mathbf{\bar x}_j \in \mathbb{R}.\] <p>With some mean centering, this is defined as the covariance. While we will continue to refer to this as covariance, we will omit all mean centering for simplicity (although it will often add a “junk component” describing a translation, <a href="/blog/2025/graph-fourier#spectra">as described previously</a>).</p> <p>But we aren’t just interested in one pair of genes; <strong>we want to look at all pairwise relationships</strong>. Let $\mathbf{\bar X} = [\mathbf{\bar x}_1 | … | \mathbf{\bar x}_g] \in \mathbb{R}^{n \times g}$ represent the cell-by-gene matrix of low-pass filtered gene signals. Then the gene-by-gene covariance matrix is given by</p> \[\mathbf{C} = \mathbf{\bar X}^{\top} \mathbf{\bar X} \in \mathbb{R}^{g \times g},\] <p>with entries $\mathbf{C}_{ij} = \mathbf{\bar x}_i^{\top} \mathbf{\bar x}_j$. We can visualize $\mathbf{C}$ as a heatmap. The genes are sorted based on their corresponding ground truth region patterns, so we expect to see interesting groups of covarying genes as blocks. (If you look closely, you’ll see that these blocks are 4x4, as there are four gene markers for each pattern.) For instance, gene markers for each region form red blocks along the diagonal. These denote positively covarying groups of genes, i.e. gene programs. Furthermore, different blocks appear to negatively covary, forming blue blocks along the <em>off</em>-diagonal. Conceptually, this just means that gene markers for each region are mutually exclusive.</p> <figure style="text-align: center;"> <img src="/assets/figures/regions/region_covariance.png" alt="" style="width:100%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 2:</strong> Visualization of the low-pass covariance matrix.</figcaption> </figure> <p>Ultimately, <strong>we seek to distill these blocks into a simpler representation</strong> – perhaps by grouping genes within related blocks into a single “gene program” describing a region. It turns out we can do this by eigendecomposing $\mathbf{C}$, i.e. performing PCA. (For a primer on PCA, see <a href="/blog/2025/pca">this post</a>.) Briefly, eigendecomposing the gene-gene covariance matrix yields the eigenbasis</p> \[\mathbf{U} = [ \mathbf{u}_1 | ... | \mathbf{u}_g ] \in \mathbb{R}^{g \times g},\] <p>where $\mathbf{u}_i \in \mathbb{R}^g$ represents the gene loadings for PC$i$. In other words, it describes each gene’s participation in gene program $i$. We can visualize this matrix as a heatmap as well. The rows represent each gene in the same order as in Figure 2. The columns now represent PCs, or “region components”, and the colors describe how much and in what direction a given gene contributes to a given component.</p> <figure style="text-align: center;"> <img src="/assets/figures/regions/region_components.png" alt="" style="width:100%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 3:</strong> Visualization of low-pass gene programs, i.e. "region components". The right-hand plot is an enlarged version of the inset in the top left corner of the left-hand plot.</figcaption> </figure> <p>The first component is entirely negative and is likely just a consequence of neglecting mean centering (the “junk component” mentioned above). The second, third, and fourth components each appear to describe relationships between regions, with each representing a positive and negative region.</p> <p>We can also visualize each component in the tissue by projecting cells onto each gene program, i.e. $\mathbf{X} \mathbf{U} \in \mathbb{R}^{n \times g}$. We end up seeing that they each describe two regions of the tissue. The first component describes the outer ring versus everything inside of it, the second component describes the second-most outer right versus everything inside, and the third component describes the third-most outer ring versus everything inside. We can also sort gene loadings to identify gene markers for each region component. This gives a set of top genes describing the “positive region” and a different set for the “negative region”. We can visualize each of these markers in the tissue as well, seeing that they are indeed expressed within the regions they describe.</p> <figure style="text-align: center;"> <img src="/assets/figures/regions/region_components_tissue.png" alt="" style="width:100%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 4:</strong> Visualization of region components and their gene markers in the tissue.</figcaption> </figure> <p>Altogether, we find that <strong>each component represents a large-scale pattern between regions</strong>. Note that the gene marker information shown in Figure 4 is the same as the information shown in Figure 3, just in a different form.</p> <h3 id="simulated-region-gradients">Simulated region gradients</h3> <p>The above components indeed appear to contain orthogonal information, as they each describe unique sets of regions. <strong>Together, however, these features might describe more complex region patterns</strong>. Rather than projecting cells onto a single component, one can instead project onto multiple components that form a multidimensional “region space”. In other words, rather than plotting cells in the tissue and <em>coloring</em> by the top three different region components as above, we can directly plot each cell in three-dimensional component space and see what shapes they form. Try interacting with the plot below to get a sense of the resulting shape.</p> <figure style="text-align: center;"> <iframe src="/assets/plotly/region_component_space.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: none;"> </iframe> <figcaption><strong>Figure 5:</strong> Region manifold visualized in region component space.</figcaption> </figure> <p><strong>The cells appear to form a one-dimensional manifold.</strong> In order to interpret this manifold, one can first fit a line to it and visualize it in tissue space. We’ll do this by <strong>treating the manifold as a domain and then finding the lowest frequency along it.</strong> We first connect neighboring cells in this region space using a weighted Delaunay triangulation. Cells closer to one another have higher weights while those further from one another have lower weights. This defines a domain in component space. We can then calculate and eigendecompose the corresponding Laplacian matrix to find the lowest eigenvector which represents the lowest frequency and thus the longest path through the domain. Note that <strong>this approach is essentially equivalent to <a href="https://en.wikipedia.org/wiki/Diffusion_map#">diffusion maps</a></strong>, which is a common single-cell trajectory inference method. Again, try interacting with the plot below to confirm that we’ve fit a line along the manifold.</p> <figure style="text-align: center;"> <iframe src="/assets/plotly/region_gradient.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: none;"> </iframe> <figcaption><strong>Figure 6:</strong> Region manifold visualized in region component space and colored by the molecular gradient it describes.</figcaption> </figure> <p><strong>The resulting line forms a gradient</strong> from one tip of the manifold to the other. Now let’s visualize this gradient in the tissue. We can take this same coloring and simply apply it to cells arranged in the tissue domain. Indeed, we find a molecularly-defined gradient that stretches from the center of the tissue outward, matching the way our tissue was constructed.</p> <figure style="text-align: center;"> <img src="/assets/figures/regions/region_gradient_tissue.png" alt="" style="width:50%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 7:</strong> Region gradient visualized in tissue space.</figcaption> </figure> <p>However, while this is a really neat way to view region features in the data, one might ultimately wish to <strong>tie this information back to the ground truth region labels</strong> we started with.</p> <h3 id="simulated-region-clusters">Simulated region clusters</h3> <p>To assign each cell to a discrete region label, one can cluster cells in region component space. We can simply perform k-means on the top three components we’ve been visualizing so far. We’ll choose $k=4$ given the number of ground truth regions in our simulated dataset. After clustering, we can return to our cells in region component space and color them by our new discrete region labels.</p> <figure style="text-align: center;"> <iframe src="/assets/plotly/region_clusters.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: none;"> </iframe> <figcaption><strong>Figure 8:</strong> Region manifold visualized in region component space and colored by discrete region clusters.</figcaption> </figure> <p>It appears that <strong>the resulting clusters partition cells along the gradient</strong>. Once again, we can take this coloring and apply it to our cells in tissue space.</p> <figure style="text-align: center;"> <img src="/assets/figures/regions/region_clusters_tissue.png" alt="" style="width:50%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 9:</strong> Region clusters visualized in tissue space.</figcaption> </figure> <p>As expected, we end up with the ground truth region clusters.</p> <hr/> <h2 id="mouse-brain">Mouse brain</h2> <p>Now <strong>let’s see how well the above analysis generalizes to real tissues</strong>, using <a href="https://www.nature.com/articles/s41586-021-03705-x">the mouse MOp</a> as an example like in the <a href="/blog/2025/graph-fourier/">last post</a>. Since it’s the inspiration for our simulation, we can just repeat each of the above analyses to identify region components, gradients, and clusters.</p> <p>Let’s start by visualizing our <a href="/blog/2025/graph-fourier#one-sample">previous low-pass filtering results</a> to build intuition. Using the slider, note how low-pass filtering isolates the large-scale components of the gene expression pattern. The gene visualized here is <em>Cux2</em>, which is a canonical neocortical layer marker.</p> <div style="width: 50%; max-width: 768px; margin: 0 auto;"> <img-comparison-slider class="slider-with-shadows"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/fourier/tissue_before_filtering_mop-480.webp 480w,/assets/figures/fourier/tissue_before_filtering_mop-800.webp 800w,/assets/figures/fourier/tissue_before_filtering_mop-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/fourier/tissue_before_filtering_mop.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/fourier/tissue_after_lowpass_mop-480.webp 480w,/assets/figures/fourier/tissue_after_lowpass_mop-800.webp 800w,/assets/figures/fourier/tissue_after_lowpass_mop-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/fourier/tissue_after_lowpass_mop.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> </div> <figcaption><strong>Figure 10:</strong> Comparison of a gene expression signal before (left image) and after (right image) low-pass filtering. </figcaption> <p><br/></p> <p>After applying the same filter to all genes in the dataset, we can perform the eigendecomposition described above to obtain region components.</p> <h3 id="brain-region-components">Brain region components</h3> <p>We find that <strong>the resulting components indeed show an interesting layered pattern</strong> that’s supported by their corresponding gene markers. However, there’s an interesting pattern that didn’t emerge in our simulated data. Notice how the region components themselves appear like a <a href="/blog/2025/graph-fourier#frequencies">frequency basis</a> along an axis from the upper to lower edges of the tissue. Earlier components represent larger-scale fluctuations, while later components represent smaller-scale fluctuations. These fluctuations occur along an implicit axis along the depth of the tissue, suggesting the presence of a molecular gradient. While this pattern won’t change our gradient identification approach going forward, I found it really beautiful and figured it was worth pointing out.</p> <figure style="text-align: center;"> <img src="/assets/figures/regions/mop/region_features.png" alt="" style="width:100%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 11:</strong> Visualization of region components and their gene markers in the tissue.</figcaption> </figure> <h3 id="brain-region-gradients">Brain region gradients</h3> <p>To calculate the molecular gradient, we’ll follow the same approach as earlier – namely <strong>treating the manifold as a domain and finding the lowest-frequency component along it.</strong> Unfortunately, the amount of data prohibits interactive 3D visualization. But we can still plot the cells in component space from multiple angles to get a sense of their latent shape. As in our simulation, it seems they form a one-dimensional manifold, and the resulting gradient coloring appears to describe the change along that manifold from end to end.</p> <figure style="text-align: center;"> <img src="/assets/figures/regions/mop/region_gradients.png" alt="" style="width:100%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 12:</strong> Region gradient visualized in region component and tissue spaces.</figcaption> </figure> <p>When plotted in the tissue, the gradient appears to run between the outer and inner layers of the neocortex. Thus, it appears <strong>our gradient identification approach applies to real biological data as well.</strong> However, to make the anatomical layout of this data clearer, we can assign discrete region labels and see if they reflect the known anatomy.</p> <h3 id="brain-region-clusters">Brain region clusters</h3> <p>Just as in our simulation, <strong>we can apply k-means to the top three components visualized above to assign each cell a region label.</strong> Unlike in our simulation, however, we don’t know the ground truth number of regions that should inform our choice of $k$. Nevertheless, we can approximate it based on the <a href="https://atlas.brain-map.org/atlas?atlas=1&amp;plate=100960348">known anatomy</a>, in which four cortical layers named L2/3, L4, L5, and L6 are sandwiched between the meninges above and white matter (WM) below. Thus, we will set $k=6$, perform k-means, and expect to find a similar set of regions.</p> <p>One way to compare our region clusters to the known anatomy is based on their gene markers. Using <a href="https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html">Scanpy’s standard one-vs-all t-test-based approach</a>, we can identify the top three gene markers for each region and compare them to the known anatomy with references such as <a href="https://cellxgene.cziscience.com/">CELLxGENE</a>. This results in a simple annotation of each region that matches the literature quite well. Additionally, note that the markers for one region appear to bleed into the next region. This is likely due to the low-pass nature of the data, in which the boundaries between regions are blurred. Finally, we can plot the resulting region labels in the tissue domain to find that <strong>they form layers characteristic of the neocortex</strong>, further validating our approach.</p> <figure style="text-align: center;"> <img src="/assets/figures/regions/mop/region_clusters.png" alt="" style="width:100%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 13:</strong> Region clusters visualized in terms of their gene markers and in tissue space.</figcaption> </figure> <hr/> <h2 id="human-lymph-node">Human lymph node</h2> <p>The above analysis is by no means restricted to tissues with layered structures. It should, in principle, be <strong>applicable to any tissue with any arrangement of molecularly-distinct regions</strong>. To test that, we will use a new dataset that was collected from a different organ, from a different species, and using a different spatial transcriptomic technology. Our new dataset is a sample of the healthy human lymph node and consists of 377 genes measured across 377,957 cells measured using <a href="https://www.biorxiv.org/content/10.1101/2023.12.07.570603v2">Xenium</a>.</p> <p>Let’s briefly introduce <strong>a bit of <a href="https://openstax.org/books/anatomy-and-physiology-2e/pages/21-1-anatomy-of-the-lymphatic-and-immune-systems">biological background</a>.</strong> Broadly, the immune system is tasked with recognizing and eliminating potentially harmful cells or molecules in the body, e.g. bacteria or viruses. A special subset of cells are dedicated toward learning to recognize these “antigens”. They do so through an evolutionary process. Each B and T cell has a random, unique receptor shape that it can use to bind to (i.e. recognize) antigens. Dendritic cells then find antigens and show them to B and T cells to see which ones can recognize them. Only the cells that recognize a given antigen survive and go on to proliferate and fight the underlying infection. <strong>Lymph nodes serve as hubs for antigens, dendritic cells, B cells, and T cells all to aggregate and interact.</strong> Its anatomy is split into separate zones such as the cortex – where B cells are “educated” – and the paracortex – where T cells are educated. This is a crude overview, but it should provide enough context going forward in this series of posts.</p> <p>We are interested in finding these anatomical regions using the above approach. The first step is performing low-pass filtering to isolate large-scale patterns. Here’s another interactive demonstration of filtering, this time in the human lymph node and showing a T cell marker gene called <em>TRAC</em>.</p> <div style="width: 40%; max-width: 768px; margin: 0 auto;"> <img-comparison-slider class="slider-with-shadows"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/regions/hln/tissue_nofilter-480.webp 480w,/assets/figures/regions/hln/tissue_nofilter-800.webp 800w,/assets/figures/regions/hln/tissue_nofilter-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/regions/hln/tissue_nofilter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/regions/hln/tissue_lowpass-480.webp 480w,/assets/figures/regions/hln/tissue_lowpass-800.webp 800w,/assets/figures/regions/hln/tissue_lowpass-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/regions/hln/tissue_lowpass.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> </div> <figcaption><strong>Figure 14:</strong> Comparison of a gene expression signal before (left image) and after (right image) low-pass filtering. </figcaption> <p><br/></p> <p>We can see that filtering emphasizes the large-scale structure over the tissue, such as the upside-down cup shape toward the center. As this is a T cell marker, we would anticipate that these patterns correspond to the paracortex. <strong>We’ll find that out by briefly identifying region components and clusters</strong> (and ignoring gradients since they aren’t as prominent as in the mouse brain).</p> <h3 id="lymph-node-region-components">Lymph node region components</h3> <p>One component in particular stands out when performing the above analysis on our human lymph node dataset. Ignoring the first translation component, <strong>the top region component appears to capture the paracortex</strong> in red and another region in blue. This blue region is likely the cortex, which is characterized by B cell “follicles”. These guesses are validated by the top gene markers, which are canonical T and B cell markers and overlap with the observed areas.</p> <figure style="text-align: center;"> <img src="/assets/figures/regions/hln/region_features.png" alt="" style="width:100%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 15:</strong> Visualization of region components and their gene markers in the tissue.</figcaption> </figure> <p>While this component alone seems to capture much of the lymph node anatomy, we can perform clustering on the top three components to assign discrete region labels and get a clearer sense of the anatomy.</p> <h3 id="lymph-node-region-clusters">Lymph node region clusters</h3> <p>As above, we will perform clustering by applying k-means to our cells in component space. Canonical lymph node anatomy suggests that we should choose $k=3$ to reflect the cortex, paracortex, and medulla (which is basically just vasculature and connective tissue outside the other two regions). The resulting clusters yield distinct markers of each region, including T cell markers in the paracortex, B cell markers in the cortex, and markers of vasculature in the medulla. In the tissue, these regions appear to represent a discretized version of the above components.</p> <figure style="text-align: center;"> <img src="/assets/figures/regions/hln/region_clusters.png" alt="" style="width:100%; display: block; margin: 0 auto;"/> <figcaption><strong>Figure 16:</strong> Region clusters visualized in terms of their gene markers and in tissue space.</figcaption> </figure> <p>Altogether, it appears <strong>we’ve identified the proper lymph node anatomy.</strong></p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>We did it. <strong>We provided a mathematical framework for characterizing regions and even made some cool connections to PCA and diffusion maps along the way.</strong> But it’s at this point that I find myself asking the painful question: <strong>who even cares about regions?</strong> Without a wealth of disease-control or spatial perturbation data to compare, they’re just blobs of cells that aren’t explicitly tied to a mechanism. What we really care about are the interactions inside of them, since they represent the molecular forces that determine tissue structure and function. So we should instead be shouting the question: <strong>HOW CAN WE REPRESENT INTERACTIONS?</strong> Furthermore, after exploring low frequencies this whole time, one might be yelling: <strong>WHAT DO <em>HIGH</em>-FREQUENCIES MEAN?</strong> In the next post, we’ll show that these two questions are deeply related.</p>]]></content><author><name>Kamal Maher</name></author><category term="spatial-omics"/><summary type="html"><![CDATA[A principled approach to representing multicellular regions in spatial omics data]]></summary></entry></feed>